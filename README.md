# Build-Basic-Generative-Adversarial-Networks-GANs-
## References

### Week 1
Curious to see people who were generated by a GAN? Check it out! [https://www.thispersondoesnotexist.com/](https://www.thispersondoesnotexist.com/)

Analyzing and Improving the Image Quality of StyleGAN [https://arxiv.org/abs/1912.04958](https://arxiv.org/abs/1912.04958)

Explore some cool GANs in an interactive way hereâ€”over the GANs specialization, you'll learn how these work and how you might apply them! [https://colab.research.google.com/github/https-deeplearning-ai/GANs-Public/blob/master/C1W1_(Colab)_Pre_trained_model_exploration.ipynb](https://colab.research.google.com/github/https-deeplearning-ai/GANs-Public/blob/master/C1W1_(Colab)_Pre_trained_model_exploration.ipynb
)

Tensorflow's implementations come with some inherent limitations highlighted in the [article](https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/).


#### From the videos:

Hyperspherical Variational Auto-Encoders (Davidson, Falorsi, De Cao, Kipf, and Tomczak, 2018):[https://www.researchgate.net/figure/Latent-space-visualization-of-the-10-MNIST-digits-in-2-dimensions-of-both-N-VAE-left_fig2_324182043](https://www.researchgate.net/figure/Latent-space-visualization-of-the-10-MNIST-digits-in-2-dimensions-of-both-N-VAE-left_fig2_324182043)

Analyzing and Improving the Image Quality of StyleGAN (Karras et al., 2020): [https://arxiv.org/abs/1912.04958](https://arxiv.org/abs/1912.04958)

Semantic Image Synthesis with Spatially-Adaptive Normalization (Park, Liu, Wang, and Zhu, 2019): [https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291)

Few-shot Adversarial Learning of Realistic Neural Talking Head Models (Zakharov, Shysheya, Burkov, and Lempitsky, 2019): [https://arxiv.org/abs/1905.08233](https://arxiv.org/abs/1905.08233)

Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling (Wu, Zhang, Xue, Freeman, and Tenenbaum, 2017): [https://arxiv.org/abs/1610.07584](https://arxiv.org/abs/1610.07584)

These Cats Do Not Exist (Glover and Mott, 2019): [http://thesecatsdonotexist.com/](http://thesecatsdonotexist.com/)

#### From the notebooks:

Large Scale GAN Training for High Fidelity Natural Image Synthesis (Brock, Donahue, and Simonyan, 2019): [https://arxiv.org/abs/1809.11096](https://arxiv.org/abs/1809.11096)

PyTorch Documentation: [https://pytorch.org/docs/stable/index.html#pytorch-documentation](https://pytorch.org/docs/stable/index.html#pytorch-documentation)

MNIST Database: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)


### Week 2

In this notebook, you're going to learn about TGAN, from the paper Temporal Generative Adversarial Nets with Singular Value Clipping [(Saito, Matsumoto, & Saito, 2017)](https://arxiv.org/pdf/1611.06624.pdf), and its origins in image generation. 

#### From the videos:

Deconvolution and Checkerboard Artifacts (Odena et al., 2016): http://doi.org/10.23915/distill.00003

#### From the notebook:

Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (Radford, Metz, and Chintala, 2016): https://arxiv.org/abs/1511.06434

MNIST Database: http://yann.lecun.com/exdb/mnist/

### Week 3

#### From the notebook:

Wasserstein GAN (Arjovsky, Chintala, and Bottou, 2017): https://arxiv.org/abs/1701.07875

Improved Training of Wasserstein GANs (Gulrajani et al., 2017): https://arxiv.org/abs/1704.00028

MNIST Database: http://yann.lecun.com/exdb/mnist/
