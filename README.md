# Build-Basic-Generative-Adversarial-Networks-GANs-
## References

### Week 1
Curious to see people who were generated by a GAN? Check it out! [https://www.thispersondoesnotexist.com/](https://www.thispersondoesnotexist.com/)

Analyzing and Improving the Image Quality of StyleGAN [https://arxiv.org/abs/1912.04958](https://arxiv.org/abs/1912.04958)

Explore some cool GANs in an interactive way hereâ€”over the GANs specialization, you'll learn how these work and how you might apply them! [https://colab.research.google.com/github/https-deeplearning-ai/GANs-Public/blob/master/C1W1_(Colab)_Pre_trained_model_exploration.ipynb](https://colab.research.google.com/github/https-deeplearning-ai/GANs-Public/blob/master/C1W1_(Colab)_Pre_trained_model_exploration.ipynb
)

Tensorflow's implementations come with some inherent limitations highlighted in the [article](https://thegradient.pub/state-of-ml-frameworks-2019-pytorch-dominates-research-tensorflow-dominates-industry/).


#### From the videos:

Hyperspherical Variational Auto-Encoders (Davidson, Falorsi, De Cao, Kipf, and Tomczak, 2018):[https://www.researchgate.net/figure/Latent-space-visualization-of-the-10-MNIST-digits-in-2-dimensions-of-both-N-VAE-left_fig2_324182043](https://www.researchgate.net/figure/Latent-space-visualization-of-the-10-MNIST-digits-in-2-dimensions-of-both-N-VAE-left_fig2_324182043)

Analyzing and Improving the Image Quality of StyleGAN (Karras et al., 2020): [https://arxiv.org/abs/1912.04958](https://arxiv.org/abs/1912.04958)

Semantic Image Synthesis with Spatially-Adaptive Normalization (Park, Liu, Wang, and Zhu, 2019): [https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291)

Few-shot Adversarial Learning of Realistic Neural Talking Head Models (Zakharov, Shysheya, Burkov, and Lempitsky, 2019): [https://arxiv.org/abs/1905.08233](https://arxiv.org/abs/1905.08233)

Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling (Wu, Zhang, Xue, Freeman, and Tenenbaum, 2017): [https://arxiv.org/abs/1610.07584](https://arxiv.org/abs/1610.07584)

These Cats Do Not Exist (Glover and Mott, 2019): [http://thesecatsdonotexist.com/](http://thesecatsdonotexist.com/)

#### From the notebooks:

Large Scale GAN Training for High Fidelity Natural Image Synthesis (Brock, Donahue, and Simonyan, 2019): [https://arxiv.org/abs/1809.11096](https://arxiv.org/abs/1809.11096)

PyTorch Documentation: [https://pytorch.org/docs/stable/index.html#pytorch-documentation](https://pytorch.org/docs/stable/index.html#pytorch-documentation)

MNIST Database: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)


### Week 2

In this notebook, you're going to learn about TGAN, from the paper Temporal Generative Adversarial Nets with Singular Value Clipping [(Saito, Matsumoto, & Saito, 2017)](https://arxiv.org/pdf/1611.06624.pdf), and its origins in image generation. 

#### From the videos:

Deconvolution and Checkerboard Artifacts (Odena et al., 2016): http://doi.org/10.23915/distill.00003

#### From the notebook:

Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks (Radford, Metz, and Chintala, 2016): https://arxiv.org/abs/1511.06434

MNIST Database: http://yann.lecun.com/exdb/mnist/

### Week 3

#### From the notebook:

Wasserstein GAN (Arjovsky, Chintala, and Bottou, 2017): https://arxiv.org/abs/1701.07875

Improved Training of Wasserstein GANs (Gulrajani et al., 2017): https://arxiv.org/abs/1704.00028

SN-GAN:  In this notebook, you'll learn about and implement spectral normalization, a weight normalization technique to stabilize the training of the discriminator, as proposed in [Spectral Normalization for Generative Adversarial Networks (Miyato et al. 2018)](https://arxiv.org/abs/1802.05957).

The goal of this notebook is to demonstrate that core GAN ideas can be applied outside of the image domain. In this notebook, you will be able to play around with a pre-trained ProteinGAN model to see how it can be used in bioinformatics to generate functional molecules.
[ProteinGAN](https://www.biorxiv.org/content/10.1101/789719v2) was developed by [Biomatters Designs](https://www.biomatterdesigns.com/) and [Zelezniak lab at Chalmers University of Technology](https://twitter.com/AZelezniak).

Want another explanation of WGAN? This article provides a great walkthrough of how WGAN addresses the difficulties of training a traditional GAN with a focus on the loss functions. From GAN to WGAN (Weng, 2017): https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html

MNIST Database: http://yann.lecun.com/exdb/mnist/

### Week 4

Conditional Generative Adversarial Nets (Mirza and Osindero, 2014): https://arxiv.org/abs/1411.1784

In this notebook, you're going to learn about InfoGAN in order to generate disentangled outputs, based on the paper, [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets by Chen et. al.](https://arxiv.org/abs/1606.03657) While there are many approaches to disentanglement, this is one of the more widely used and better known.

#### From the videos:

Interpreting the Latent Space of GANs for Semantic Face Editing (Shen, Gu, Tang, and Zhou, 2020): https://arxiv.org/abs/1907.10786

#### From the notebooks:

MNIST Database: http://yann.lecun.com/exdb/mnist/

CelebFaces Attributes Dataset (CelebA): http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html

Seven Loss Functions that includes the Minimax loss (MM GAN), Non-Saturating loss (NS GAN), Wasserstein loss (WGAN), and Least-Squares loss (LS GAN) described above. The study also includes an extension of Wasserstein loss to remove the weight clipping called Wasserstein Gradient Penalty loss (WGAN GP) and two others, DRAGAN and BEGAN. Are GANs Created Equal? [A Large-Scale Study, 2018.](https://arxiv.org/abs/1711.10337)


